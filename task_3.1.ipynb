{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a1fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def find_point(value, increment):\n",
    "    return round(round(value / increment) * increment, 2)\n",
    "\n",
    "def approximate(raw_state):\n",
    "    array = list(raw_state)\n",
    "    return tuple([\n",
    "        find_point(array[0], 0.2) + 0, \n",
    "        find_point(array[1], 0.2) + 0, \n",
    "        find_point(array[2], 1) + 0,\n",
    "        find_point(array[3], 2) + 0\n",
    "    ])\n",
    "\n",
    "def generate(lower_bound, upper_bound, interval):\n",
    "    output = [lower_bound]\n",
    "    while lower_bound <= upper_bound:\n",
    "        lower_bound += interval\n",
    "        output.append( float( round(lower_bound,1) + 0 ) )\n",
    "    return output\n",
    "\n",
    "def generate_states(theta1, theta2, ang_vel_theta1, ang_vel_theta2):\n",
    "    states = []\n",
    "    for a in theta1:\n",
    "        for b in theta2:\n",
    "            for c in ang_vel_theta1:\n",
    "                for d in ang_vel_theta2:\n",
    "                    states.append((a, b, c, d))\n",
    "    return states\n",
    "\n",
    "\n",
    "def create_transition_reward_function(states, actions, env):\n",
    "    table = {}\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            env.reset(state=state)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            table[(state, action)] = {'reward':reward, 'next_state':approximate(obs)}\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c262c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(state, actions, transition_and_reward_function, policy, value_function, gamma=0.9, max_iterations=100):\n",
    "    min_difference = 0.01\n",
    "    new_value_function = {}\n",
    "    first_iter = True\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        for state in states:\n",
    "            new_val = 0\n",
    "            for action in actions:\n",
    "                reward, next_state = transition_and_reward_function[(state, action)].values()\n",
    "                next_state_value = value_function[next_state]\n",
    "                    \n",
    "                new_val += policy[state][action] * (reward + gamma*next_state_value)\n",
    "            \n",
    "            \n",
    "            new_value_function[state] = new_val\n",
    "        \n",
    "        # Check convergence\n",
    "        if not first_iter:\n",
    "            differences = [abs(new_value_function[state] - value_function[state])\n",
    "               for state in states]\n",
    "            \n",
    "            diff = max(differences)\n",
    "            print(f\"policy_evaluation iteration_number: {i}. Diff: {diff}\")\n",
    "            \n",
    "            if diff < min_difference:\n",
    "                print(f\"Policy Evaluation converged at {i}\")\n",
    "                return new_value_function\n",
    "                \n",
    "        else:\n",
    "            first_iter = False\n",
    "        \n",
    "        value_function = new_value_function.copy()\n",
    "            \n",
    "    print(f\"Policy Evaluation did not converge.\")\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def improve_policy(states, actions, transition_and_reward_function, value_function, gamma=0.9):\n",
    "    new_policy = {}\n",
    "    for state in states:\n",
    "        action_values = {}\n",
    "        for action in actions:\n",
    "            reward, next_state = transition_and_reward_function[(state, action)].values()\n",
    "            action_values[action] = reward + gamma * value_function[next_state]\n",
    "        greedy_action, value = max(action_values.items(), key= lambda pair: pair[1])\n",
    "        \n",
    "        new_policy[state] = {action:1 if action is greedy_action else 0 for action in actions}\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(states, actions, transition_and_reward_function, policy, value_function, max_iterations=200):\n",
    "    new_value_function = {}\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        converged = True\n",
    "        print(\"policy iteration_number:\",i)\n",
    "        \n",
    "        # Evaluate the current policy\n",
    "        new_value_function = policy_evaluation(states, actions, transition_and_reward_function, policy, value_function)\n",
    "        \n",
    "        # Improve the policy based on new value function\n",
    "        new_policy = improve_policy(states, actions, transition_and_reward_function, new_value_function)\n",
    "        value_function = new_value_function.copy()\n",
    "        \n",
    "        # Check if convergence\n",
    "        for state in states:\n",
    "            if get_optimal_action(state, policy) != get_optimal_action(state, new_policy):\n",
    "                policy = new_policy.copy()\n",
    "                converged = False\n",
    "                break\n",
    "        \n",
    "        if converged:\n",
    "            # we have convergence\n",
    "            print(f\"Policy Iteration converged at {i}\")\n",
    "            return new_policy\n",
    "\n",
    "    print(f\"Policy Iteration did not converge\")\n",
    "    return policy\n",
    "\n",
    "def get_optimal_action(state, optimal_policy):\n",
    "    greedy_action, prob = max(optimal_policy[state].items(), key= lambda pair: pair[1])\n",
    "    return greedy_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385a47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acrobot_env import *\n",
    "\n",
    "env = AcrobotEnv()\n",
    "\n",
    "theta1 = generate(-6.2, 6.2, 0.2)\n",
    "theta2 = generate(-6.2, 6.2, 0.2)\n",
    "ang_vel_theta1 = generate(-13, 13, 1)\n",
    "ang_vel_theta2 = generate(-28, 28, 2)\n",
    "\n",
    "states = generate_states(theta1, theta2, ang_vel_theta1, ang_vel_theta2)\n",
    "actions = [0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff779d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333960"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2590f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "load_transition = False\n",
    "\n",
    "if not load_transition:\n",
    "    transition_and_reward_function = create_transition_reward_function(states, actions, env)\n",
    "    dump(transition_and_reward_function, 'transition_and_reward_function.joblib') \n",
    "else:\n",
    "    print(\"Loading transition and reward function\")\n",
    "    transition_and_reward_function = load('transition_and_reward_function.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f07ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration_number: 0\n",
      "policy_evaluation iteration_number: 1. Diff: 0.8820900000000003\n",
      "policy_evaluation iteration_number: 2. Diff: 0.7859421900000008\n",
      "policy_evaluation iteration_number: 3. Diff: 0.7002744912900014\n",
      "policy_evaluation iteration_number: 4. Diff: 0.6239445717393917\n",
      "policy_evaluation iteration_number: 5. Diff: 0.5559346134197991\n",
      "policy_evaluation iteration_number: 6. Diff: 0.4953377405570425\n",
      "policy_evaluation iteration_number: 7. Diff: 0.44134592683632423\n",
      "policy_evaluation iteration_number: 8. Diff: 0.39323922081116613\n",
      "policy_evaluation iteration_number: 9. Diff: 0.3503761457427501\n",
      "policy_evaluation iteration_number: 10. Diff: 0.312185145856791\n",
      "policy_evaluation iteration_number: 11. Diff: 0.27815696495840214\n",
      "policy_evaluation iteration_number: 12. Diff: 0.24783785577793616\n",
      "policy_evaluation iteration_number: 13. Diff: 0.22082352949814155\n",
      "policy_evaluation iteration_number: 14. Diff: 0.19675376478284434\n",
      "policy_evaluation iteration_number: 15. Diff: 0.17530760442151472\n",
      "policy_evaluation iteration_number: 16. Diff: 0.15619907553957013\n",
      "policy_evaluation iteration_number: 17. Diff: 0.13917337630575677\n",
      "policy_evaluation iteration_number: 18. Diff: 0.12400347828843117\n",
      "policy_evaluation iteration_number: 19. Diff: 0.11048709915499266\n",
      "policy_evaluation iteration_number: 20. Diff: 0.0984440053470994\n",
      "policy_evaluation iteration_number: 21. Diff: 0.0877136087642647\n",
      "policy_evaluation iteration_number: 22. Diff: 0.07815282540896007\n",
      "policy_evaluation iteration_number: 23. Diff: 0.06963416743938389\n",
      "policy_evaluation iteration_number: 24. Diff: 0.06204404318849033\n",
      "policy_evaluation iteration_number: 25. Diff: 0.055281242480946347\n",
      "policy_evaluation iteration_number: 26. Diff: 0.049255587050522465\n",
      "policy_evaluation iteration_number: 27. Diff: 0.043886728062015656\n",
      "policy_evaluation iteration_number: 28. Diff: 0.03910307470325591\n",
      "policy_evaluation iteration_number: 29. Diff: 0.034840839560601466\n",
      "policy_evaluation iteration_number: 30. Diff: 0.031043188048496262\n",
      "policy_evaluation iteration_number: 31. Diff: 0.027659480551210258\n",
      "policy_evaluation iteration_number: 32. Diff: 0.02464459717112799\n",
      "policy_evaluation iteration_number: 33. Diff: 0.02195833607947506\n",
      "policy_evaluation iteration_number: 34. Diff: 0.01956487744681379\n",
      "policy_evaluation iteration_number: 35. Diff: 0.01743230580511046\n",
      "policy_evaluation iteration_number: 36. Diff: 0.01553218447235416\n",
      "policy_evaluation iteration_number: 37. Diff: 0.013839176364868422\n",
      "policy_evaluation iteration_number: 38. Diff: 0.012330706141097991\n",
      "policy_evaluation iteration_number: 39. Diff: 0.010986659171718927\n",
      "policy_evaluation iteration_number: 40. Diff: 0.009789113322002052\n",
      "Policy Evaluation converged at 40\n",
      "policy iteration_number: 1\n",
      "policy_evaluation iteration_number: 1. Diff: 3.504954456914632\n",
      "policy_evaluation iteration_number: 2. Diff: 3.1544590112231687\n",
      "policy_evaluation iteration_number: 3. Diff: 2.8390131101008524\n",
      "policy_evaluation iteration_number: 4. Diff: 2.555111799090767\n",
      "policy_evaluation iteration_number: 5. Diff: 2.2996006191816907\n",
      "policy_evaluation iteration_number: 6. Diff: 2.0696405572635213\n",
      "policy_evaluation iteration_number: 7. Diff: 1.8626765015371678\n",
      "policy_evaluation iteration_number: 8. Diff: 1.6764088513834512\n",
      "policy_evaluation iteration_number: 9. Diff: 1.508767966245106\n",
      "policy_evaluation iteration_number: 10. Diff: 1.0061007671525637\n",
      "policy_evaluation iteration_number: 11. Diff: 0.9054906904373059\n",
      "policy_evaluation iteration_number: 12. Diff: 0.8149416213935741\n",
      "policy_evaluation iteration_number: 13. Diff: 0.7334474592542168\n",
      "policy_evaluation iteration_number: 14. Diff: 0.6601027133287953\n",
      "policy_evaluation iteration_number: 15. Diff: 0.4577262438420142\n",
      "policy_evaluation iteration_number: 16. Diff: 0.4119536194578135\n",
      "policy_evaluation iteration_number: 17. Diff: 0.33935790112096953\n",
      "policy_evaluation iteration_number: 18. Diff: 0.3007835729675339\n",
      "policy_evaluation iteration_number: 19. Diff: 0.24118999543568265\n",
      "policy_evaluation iteration_number: 20. Diff: 0.20040082211924926\n",
      "policy_evaluation iteration_number: 21. Diff: 0.1775724184192473\n",
      "policy_evaluation iteration_number: 22. Diff: 0.1598151765773217\n",
      "policy_evaluation iteration_number: 23. Diff: 0.14383365891958988\n",
      "policy_evaluation iteration_number: 24. Diff: 0.12764748504375234\n",
      "policy_evaluation iteration_number: 25. Diff: 0.11488273653937675\n",
      "policy_evaluation iteration_number: 26. Diff: 0.10339446288543996\n",
      "policy_evaluation iteration_number: 27. Diff: 0.08936483994546496\n",
      "policy_evaluation iteration_number: 28. Diff: 0.0778220343829048\n",
      "policy_evaluation iteration_number: 29. Diff: 0.07003983094461397\n",
      "policy_evaluation iteration_number: 30. Diff: 0.06303584785015381\n",
      "policy_evaluation iteration_number: 31. Diff: 0.0567322630651379\n",
      "policy_evaluation iteration_number: 32. Diff: 0.05105903675862322\n",
      "policy_evaluation iteration_number: 33. Diff: 0.045953133082759834\n",
      "policy_evaluation iteration_number: 34. Diff: 0.04135781977448438\n",
      "policy_evaluation iteration_number: 35. Diff: 0.03719206423701227\n",
      "policy_evaluation iteration_number: 36. Diff: 0.03347285781331166\n",
      "policy_evaluation iteration_number: 37. Diff: 0.030125572031980852\n",
      "policy_evaluation iteration_number: 38. Diff: 0.02711301482878259\n",
      "policy_evaluation iteration_number: 39. Diff: 0.02440171334590424\n",
      "policy_evaluation iteration_number: 40. Diff: 0.021961542011314172\n",
      "policy_evaluation iteration_number: 41. Diff: 0.019765387810183555\n",
      "policy_evaluation iteration_number: 42. Diff: 0.017788849029166443\n",
      "policy_evaluation iteration_number: 43. Diff: 0.016009964126249443\n",
      "policy_evaluation iteration_number: 44. Diff: 0.014408967713624321\n",
      "policy_evaluation iteration_number: 45. Diff: 0.012968070942261711\n",
      "policy_evaluation iteration_number: 46. Diff: 0.011671263848036517\n",
      "policy_evaluation iteration_number: 47. Diff: 0.010504137463232865\n",
      "policy_evaluation iteration_number: 48. Diff: 0.009453723716909224\n",
      "Policy Evaluation converged at 48\n",
      "policy iteration_number: 2\n",
      "policy_evaluation iteration_number: 1. Diff: 4.107013251701915\n",
      "policy_evaluation iteration_number: 2. Diff: 3.696311926531723\n",
      "policy_evaluation iteration_number: 3. Diff: 2.8882255219455297\n",
      "policy_evaluation iteration_number: 4. Diff: 2.599402969750977\n",
      "policy_evaluation iteration_number: 5. Diff: 2.3394626727758787\n",
      "policy_evaluation iteration_number: 6. Diff: 2.1055164054982916\n",
      "policy_evaluation iteration_number: 7. Diff: 1.8949647649484627\n",
      "policy_evaluation iteration_number: 8. Diff: 1.7054682884536163\n",
      "policy_evaluation iteration_number: 9. Diff: 1.5349214596082552\n",
      "policy_evaluation iteration_number: 10. Diff: 1.3814293136474296\n",
      "policy_evaluation iteration_number: 11. Diff: 1.2432863822826867\n",
      "policy_evaluation iteration_number: 12. Diff: 1.118957744054418\n",
      "policy_evaluation iteration_number: 13. Diff: 1.0070619696489764\n",
      "policy_evaluation iteration_number: 14. Diff: 0.9063557726840781\n",
      "policy_evaluation iteration_number: 15. Diff: 0.8157201954156719\n",
      "policy_evaluation iteration_number: 16. Diff: 0.734148175874104\n",
      "policy_evaluation iteration_number: 17. Diff: 0.6607333582866932\n",
      "policy_evaluation iteration_number: 18. Diff: 0.5946600224580241\n",
      "policy_evaluation iteration_number: 19. Diff: 0.5351940202122201\n",
      "policy_evaluation iteration_number: 20. Diff: 0.48167461819099877\n",
      "policy_evaluation iteration_number: 21. Diff: 0.3673797771535092\n",
      "policy_evaluation iteration_number: 22. Diff: 0.3306417994381583\n",
      "policy_evaluation iteration_number: 23. Diff: 0.29757761949434425\n",
      "policy_evaluation iteration_number: 24. Diff: 0.2678198575449091\n",
      "policy_evaluation iteration_number: 25. Diff: 0.24103787179041802\n",
      "policy_evaluation iteration_number: 26. Diff: 0.21693408461137587\n",
      "policy_evaluation iteration_number: 27. Diff: 0.1939784099647932\n",
      "policy_evaluation iteration_number: 28. Diff: 0.17458056896831398\n",
      "policy_evaluation iteration_number: 29. Diff: 0.1571225120714832\n",
      "policy_evaluation iteration_number: 30. Diff: 0.14141026086433506\n",
      "policy_evaluation iteration_number: 31. Diff: 0.1272692347779012\n",
      "policy_evaluation iteration_number: 32. Diff: 0.11454231130011117\n",
      "policy_evaluation iteration_number: 33. Diff: 0.1030880801701004\n",
      "policy_evaluation iteration_number: 34. Diff: 0.09277927215309134\n",
      "policy_evaluation iteration_number: 35. Diff: 0.08350134493778327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_evaluation iteration_number: 36. Diff: 0.0751512104440053\n",
      "policy_evaluation iteration_number: 37. Diff: 0.06763608939960442\n",
      "policy_evaluation iteration_number: 38. Diff: 0.06087248045964344\n",
      "policy_evaluation iteration_number: 39. Diff: 0.054785232413678386\n",
      "policy_evaluation iteration_number: 40. Diff: 0.049306709172310725\n",
      "policy_evaluation iteration_number: 41. Diff: 0.04437603825508063\n",
      "policy_evaluation iteration_number: 42. Diff: 0.03993843442957257\n",
      "policy_evaluation iteration_number: 43. Diff: 0.035944590986614955\n",
      "policy_evaluation iteration_number: 44. Diff: 0.03235013188795399\n",
      "policy_evaluation iteration_number: 45. Diff: 0.02911511869915895\n",
      "policy_evaluation iteration_number: 46. Diff: 0.026203606829243498\n",
      "policy_evaluation iteration_number: 47. Diff: 0.02358324614631968\n",
      "policy_evaluation iteration_number: 48. Diff: 0.021224921531688423\n",
      "policy_evaluation iteration_number: 49. Diff: 0.01910242937851958\n",
      "policy_evaluation iteration_number: 50. Diff: 0.01719218644066789\n",
      "policy_evaluation iteration_number: 51. Diff: 0.015472967796601012\n",
      "policy_evaluation iteration_number: 52. Diff: 0.0139256710169402\n",
      "policy_evaluation iteration_number: 53. Diff: 0.012533103915245647\n",
      "policy_evaluation iteration_number: 54. Diff: 0.011279793523723214\n",
      "policy_evaluation iteration_number: 55. Diff: 0.01015181417135036\n",
      "policy_evaluation iteration_number: 56. Diff: 0.009136632754215057\n",
      "Policy Evaluation converged at 56\n",
      "policy iteration_number: 3\n",
      "policy_evaluation iteration_number: 1. Diff: 2.819722309346801\n",
      "policy_evaluation iteration_number: 2. Diff: 2.5377500784121203\n",
      "policy_evaluation iteration_number: 3. Diff: 2.283975070570908\n",
      "policy_evaluation iteration_number: 4. Diff: 2.0555775635138174\n",
      "policy_evaluation iteration_number: 5. Diff: 1.5840749429178915\n",
      "policy_evaluation iteration_number: 6. Diff: 1.4256674486261023\n",
      "policy_evaluation iteration_number: 7. Diff: 1.283100703763492\n",
      "policy_evaluation iteration_number: 8. Diff: 1.154790633387143\n",
      "policy_evaluation iteration_number: 9. Diff: 1.0393115700484286\n",
      "policy_evaluation iteration_number: 10. Diff: 0.9353804130435854\n",
      "policy_evaluation iteration_number: 11. Diff: 0.8418423717392267\n",
      "policy_evaluation iteration_number: 12. Diff: 0.7576581345653048\n",
      "policy_evaluation iteration_number: 13. Diff: 0.6427651441141515\n",
      "policy_evaluation iteration_number: 14. Diff: 0.5784886297027363\n",
      "policy_evaluation iteration_number: 15. Diff: 0.5206397667324634\n",
      "policy_evaluation iteration_number: 16. Diff: 0.4685757900592167\n",
      "policy_evaluation iteration_number: 17. Diff: 0.34159015098606726\n",
      "policy_evaluation iteration_number: 18. Diff: 0.2667776480062516\n",
      "policy_evaluation iteration_number: 19. Diff: 0.2400998832056267\n",
      "policy_evaluation iteration_number: 20. Diff: 0.20667104394956848\n",
      "policy_evaluation iteration_number: 21. Diff: 0.18600393955461136\n",
      "policy_evaluation iteration_number: 22. Diff: 0.16740354559915005\n",
      "policy_evaluation iteration_number: 23. Diff: 0.15066319103923576\n",
      "policy_evaluation iteration_number: 24. Diff: 0.1355968719353129\n",
      "policy_evaluation iteration_number: 25. Diff: 0.1220371847417816\n",
      "policy_evaluation iteration_number: 26. Diff: 0.10983346626760593\n",
      "policy_evaluation iteration_number: 27. Diff: 0.09885011964084534\n",
      "policy_evaluation iteration_number: 28. Diff: 0.08896510767676169\n",
      "policy_evaluation iteration_number: 29. Diff: 0.08006859690908108\n",
      "policy_evaluation iteration_number: 30. Diff: 0.07206173721817333\n",
      "policy_evaluation iteration_number: 31. Diff: 0.06233289196448055\n",
      "policy_evaluation iteration_number: 32. Diff: 0.0560996027680325\n",
      "policy_evaluation iteration_number: 33. Diff: 0.05048964249122889\n",
      "policy_evaluation iteration_number: 34. Diff: 0.04544067824210618\n",
      "policy_evaluation iteration_number: 35. Diff: 0.04089661041789583\n",
      "policy_evaluation iteration_number: 36. Diff: 0.03680694937610696\n",
      "policy_evaluation iteration_number: 37. Diff: 0.03312625443849537\n",
      "policy_evaluation iteration_number: 38. Diff: 0.029813628994646457\n",
      "policy_evaluation iteration_number: 39. Diff: 0.026832266095182078\n",
      "policy_evaluation iteration_number: 40. Diff: 0.024149039485664048\n",
      "policy_evaluation iteration_number: 41. Diff: 0.021734135537098354\n",
      "policy_evaluation iteration_number: 42. Diff: 0.019560721983388873\n",
      "policy_evaluation iteration_number: 43. Diff: 0.017604649785048387\n",
      "policy_evaluation iteration_number: 44. Diff: 0.015844184806542927\n",
      "policy_evaluation iteration_number: 45. Diff: 0.01425976632588899\n",
      "policy_evaluation iteration_number: 46. Diff: 0.012833789693301512\n",
      "policy_evaluation iteration_number: 47. Diff: 0.011550410723970117\n",
      "policy_evaluation iteration_number: 48. Diff: 0.010395369651574171\n",
      "policy_evaluation iteration_number: 49. Diff: 0.009355832686416932\n",
      "Policy Evaluation converged at 49\n",
      "policy iteration_number: 4\n",
      "policy_evaluation iteration_number: 1. Diff: 1.7628062250173753\n",
      "policy_evaluation iteration_number: 2. Diff: 1.5865256025156376\n",
      "policy_evaluation iteration_number: 3. Diff: 1.4050563955944382\n",
      "policy_evaluation iteration_number: 4. Diff: 1.2645507560349936\n",
      "policy_evaluation iteration_number: 5. Diff: 1.0242860932031723\n",
      "policy_evaluation iteration_number: 6. Diff: 0.9218574838828548\n",
      "policy_evaluation iteration_number: 7. Diff: 0.8296717354945686\n",
      "policy_evaluation iteration_number: 8. Diff: 0.746704561945112\n",
      "policy_evaluation iteration_number: 9. Diff: 0.6720341057506003\n",
      "policy_evaluation iteration_number: 10. Diff: 0.60483069517554\n",
      "policy_evaluation iteration_number: 11. Diff: 0.544347625657986\n",
      "policy_evaluation iteration_number: 12. Diff: 0.4899128630921874\n",
      "policy_evaluation iteration_number: 13. Diff: 0.4409215767829684\n",
      "policy_evaluation iteration_number: 14. Diff: 0.3968294191046713\n",
      "policy_evaluation iteration_number: 15. Diff: 0.35714647719420434\n",
      "policy_evaluation iteration_number: 16. Diff: 0.32143182947478444\n",
      "policy_evaluation iteration_number: 17. Diff: 0.28928864652730546\n",
      "policy_evaluation iteration_number: 18. Diff: 0.2603597818745751\n",
      "policy_evaluation iteration_number: 19. Diff: 0.23432380368711758\n",
      "policy_evaluation iteration_number: 20. Diff: 0.21089142331840627\n",
      "policy_evaluation iteration_number: 21. Diff: 0.18980228098656582\n",
      "policy_evaluation iteration_number: 22. Diff: 0.17082205288790853\n",
      "policy_evaluation iteration_number: 23. Diff: 0.15373984759911785\n",
      "policy_evaluation iteration_number: 24. Diff: 0.07178980286051662\n",
      "policy_evaluation iteration_number: 25. Diff: 0.06461082257446549\n",
      "policy_evaluation iteration_number: 26. Diff: 0.05814974031701858\n",
      "policy_evaluation iteration_number: 27. Diff: 0.05233476628531619\n",
      "policy_evaluation iteration_number: 28. Diff: 0.037857609188796104\n",
      "policy_evaluation iteration_number: 29. Diff: 0.034071848269916494\n",
      "policy_evaluation iteration_number: 30. Diff: 0.03066466344292529\n",
      "policy_evaluation iteration_number: 31. Diff: 0.02759819709863187\n",
      "policy_evaluation iteration_number: 32. Diff: 0.02483837738876815\n",
      "policy_evaluation iteration_number: 33. Diff: 0.02235453964989187\n",
      "policy_evaluation iteration_number: 34. Diff: 0.02011908568490295\n",
      "policy_evaluation iteration_number: 35. Diff: 0.018107177116412743\n",
      "policy_evaluation iteration_number: 36. Diff: 0.016296459404770758\n",
      "policy_evaluation iteration_number: 37. Diff: 0.014666813464294215\n",
      "policy_evaluation iteration_number: 38. Diff: 0.013200132117865415\n",
      "policy_evaluation iteration_number: 39. Diff: 0.008965436226530343\n",
      "Policy Evaluation converged at 39\n",
      "policy iteration_number: 5\n",
      "policy_evaluation iteration_number: 1. Diff: 1.3056941446953045\n",
      "policy_evaluation iteration_number: 2. Diff: 1.1751247302257735\n",
      "policy_evaluation iteration_number: 3. Diff: 1.0576122572031963\n",
      "policy_evaluation iteration_number: 4. Diff: 0.9518510314828772\n",
      "policy_evaluation iteration_number: 5. Diff: 0.8566659283345892\n",
      "policy_evaluation iteration_number: 6. Diff: 0.7709993355011306\n",
      "policy_evaluation iteration_number: 7. Diff: 0.6938994019510183\n",
      "policy_evaluation iteration_number: 8. Diff: 0.46720667072465893\n",
      "policy_evaluation iteration_number: 9. Diff: 0.4204860036521936\n",
      "policy_evaluation iteration_number: 10. Diff: 0.3784374032869735\n",
      "policy_evaluation iteration_number: 11. Diff: 0.34059366295827687\n",
      "policy_evaluation iteration_number: 12. Diff: 0.30653429666244847\n",
      "policy_evaluation iteration_number: 13. Diff: 0.2758808669962036\n",
      "policy_evaluation iteration_number: 14. Diff: 0.23042762552791007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_evaluation iteration_number: 15. Diff: 0.20738486297511916\n",
      "policy_evaluation iteration_number: 16. Diff: 0.14145605489976898\n",
      "policy_evaluation iteration_number: 17. Diff: 0.12731044940979253\n",
      "policy_evaluation iteration_number: 18. Diff: 0.1145794044688131\n",
      "policy_evaluation iteration_number: 19. Diff: 0.10312146402193179\n",
      "policy_evaluation iteration_number: 20. Diff: 0.09280931761973843\n",
      "policy_evaluation iteration_number: 21. Diff: 0.05368210621575198\n",
      "policy_evaluation iteration_number: 22. Diff: 0.04831389559417687\n",
      "policy_evaluation iteration_number: 23. Diff: 0.043482506034759894\n",
      "policy_evaluation iteration_number: 24. Diff: 0.03913425543128213\n",
      "policy_evaluation iteration_number: 25. Diff: 0.03522082988815356\n",
      "policy_evaluation iteration_number: 26. Diff: 0.03169874689933838\n",
      "policy_evaluation iteration_number: 27. Diff: 0.02852887220940481\n",
      "policy_evaluation iteration_number: 28. Diff: 0.025675984988464684\n",
      "policy_evaluation iteration_number: 29. Diff: 0.02310838648961777\n",
      "policy_evaluation iteration_number: 30. Diff: 0.02079754784065546\n",
      "policy_evaluation iteration_number: 31. Diff: 0.018717793056589827\n",
      "policy_evaluation iteration_number: 32. Diff: 0.01684601375093031\n",
      "policy_evaluation iteration_number: 33. Diff: 0.01516141237583657\n",
      "policy_evaluation iteration_number: 34. Diff: 0.013645271138252646\n",
      "policy_evaluation iteration_number: 35. Diff: 0.01228074402442747\n",
      "policy_evaluation iteration_number: 36. Diff: 0.0110526696219857\n",
      "policy_evaluation iteration_number: 37. Diff: 0.002863671584426797\n",
      "Policy Evaluation converged at 37\n",
      "policy iteration_number: 6\n",
      "policy_evaluation iteration_number: 1. Diff: 1.031936389358174\n",
      "policy_evaluation iteration_number: 2. Diff: 0.928742750422356\n",
      "policy_evaluation iteration_number: 3. Diff: 0.7120967388620665\n",
      "policy_evaluation iteration_number: 4. Diff: 0.6408870649758596\n",
      "policy_evaluation iteration_number: 5. Diff: 0.5767983584782739\n",
      "policy_evaluation iteration_number: 6. Diff: 0.5191185226304462\n",
      "policy_evaluation iteration_number: 7. Diff: 0.4672066703674016\n",
      "policy_evaluation iteration_number: 8. Diff: 0.42048600333066144\n"
     ]
    }
   ],
   "source": [
    "starting_policy = {state:{0: 0.33, 1: 0.33, 2: 0.33} for state in states}\n",
    "value_function = {state:0 for state in states}\n",
    "\n",
    "optimal_policy = policy_iteration(states, actions, transition_and_reward_function, starting_policy, value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(optimal_policy, 'optimal_policy.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82625b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3000\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     action \u001b[38;5;241m=\u001b[39m get_optimal_action(approximate(observation), optimal_policy)\n\u001b[1;32m      8\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/workspaces/acrobot-policy-iteration/acrobot_env.py:312\u001b[0m, in \u001b[0;36mAcrobotEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    311\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes=1\n",
    "\n",
    "for episode in range(0,num_episodes):\n",
    "    observation = env.reset()\n",
    "    for timestep in range(1,3000):\n",
    "        env.render()\n",
    "        action = get_optimal_action(approximate(observation), optimal_policy)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            print('COMPLETED')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89cae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fee87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
